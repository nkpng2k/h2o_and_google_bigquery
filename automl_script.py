from h2o_and_google_bigquery import GoogleH2OIntegration
from google.cloud import bigquery
from h2o.automl import H2OAutoML
from sklearn.model_selection import train_test_split
import numpy as np
import h2o


dataset = 'iris_dataset'
pred_table = 'pred_table'
bq_auth = '/Users/npng/Downloads/h2o-project-090347f40536.json'
g_h2o = GoogleH2OIntegration(dataset, pred_table, bq_auth=bq_auth)
df = g_h2o.bigquery_query()

# NOTE: Do some feature engineering
#       No feature engineering has been done for this toy example

y = np.zeros((df.shape[0], 1))

X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)
df = df.drop(['Id','Species'], axis=1)

# Initialize H2O and ingest data returned by bigquery query
h2o.init(nthreads=-1, max_mem_size='2g', ip="127.0.0.1", port=54321)
train_col = list(X_train.columns)
test_col = list(X_test.columns)
train = h2o.H2OFrame.from_python(X_train, column_names=train_col)
test = h2o.H2OFrame.from_python(X_test, column_names=test_col)

# Create lists of column names to be passed to H2O AutoML
x = train.columns
y = 'Species'
ids = 'Id'
x.remove(y)
x.remove(ids)

# Create instance of H2O AutoML and search for best model.
aml = H2OAutoML(max_runtime_secs=30)
aml.train(x=x, y=y, training_frame=train, leaderboard_frame=test)
lb = aml.leaderboard
print lb

# Ingest data for prediction. Note that in this case I just used the same
# data, In reality this would be new data.
all_data = h2o.H2OFrame.from_python(df, column_names=x)
predictions = aml.leader.predict(all_data).as_data_frame()

# Zip together a test_id and predictions as list of tuples
# The test_id's here as just toy ids generated by xrange()
# [(test_id, prediction)] * n_rows
# Call bigquery client to add contents of add_to_table to previously
# created predictions_table
test_ids = xrange(predictions.shape[0])
g_h2o.write_to_table(test_ids, predictions['predict'])
